{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Binder Slideshow Instructions:\n",
    "1. Run `Kernel > Restart & Run All` (so 3D plots get generated)\n",
    "2. When that finishes, press `Alt-R` (or the little bar-graph icon in the middle of the toolbar) then `Home` to start the show! \n",
    "\n",
    "On a Mac, `Home` is `Command-LeftArrow`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Slide 0: Tips\n",
    "* Make sure your browser is in full screen mode (F11) and zoom=100%\n",
    "* Press the space bar to advance to the next slide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY1GKEo098Ym",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Joy of 3D 🎉<br>*Visualizations for Teaching about Classification*\n",
    "## Scott H. Hawley\n",
    "Department of Chemistry & Physics<br>\n",
    "College of Sciences and Mathematics\n",
    "<p style=\"font-size:75%;\">\n",
    "    Re:Teaching PHY/BSA 3895: \"Deep Learning and AI Ethics,\" Fall 2021.</p>\n",
    "<br><br><br><br><br>\n",
    "<p style=\"font-size:75%; text-align:center; color:gray\">Scholarship of Teaching &amp; Learning Symposium, Belmont University, April 28, 2021\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from mrspuff.viz import *\n",
    "from mrspuff.utils import calc_prob, one_hot, softmax\n",
    "from mrspuff.scrape import exhibit_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Intro\n",
    "If your field is related to any of these... <p style=\"font-size:200%\">🎜👩‍⚕️⚕️⚖️👮📚🎹👩‍🎤🛒☣️🧑‍🚀🧬🎥🧑‍🎓☢️🧳🧑‍🏫📦🎧🕵️♀️♂️<a href=\"https://hedges.belmont.edu/~shawley/naughty/\">🎅</a>💲📖🧑‍🍳⚗️📡👩‍🏭🔬🖧🧫🧑‍⚖️📈🎮🧑‍🌾📉💉🧑‍🔬💊🗣️♟️🎸</p>\n",
    "\n",
    "...then automated classification increasingly \"a big deal.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdzG_hRy8vlo",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation\n",
    "<br>\n",
    "<div style=\"float:left; width:60%\"><b>Classification</b> is fascinating to me. Physicists don't do it much but the rest of society relies on it.\n",
    "\n",
    "I like &amp; teach machine learning (ML), esp. deep learning (DL). *Lots* of ML is classification. How humans & machines do it differently is the topic of my *popular-level* eBook-in-progess that uses interactive visualizations (instead of \"math\").</div>\n",
    "<img src=\"https://thumbor.forbes.com/thumbor/960x0/https%3A%2F%2Fblogs-images.forbes.com%2Folliebarder%2Ffiles%2F2018%2F05%2Fpigeon_meme_fighbird-1200x806.jpg\" style=\"float:right; width:35%\"><br>\n",
    "\n",
    "<div style=\"float:left;\"><br>Recently many research papers (e.g., ML-audio) use \"contrastive losses\" -- ??<br>\n",
    "Turns out you can relate these to traditional (DL-based) classification if you view the latter a <i>certain way</i> -- \"view\" being the operative word!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**TODO:shorten** We'll compare & contrast \"traditional\" machine learning (ML) classification with so-called \"zero-shot\" classifiers that rely on [embedding](https://arxiv.org/abs/1604.06737) semantically meaningful features as clusters in space by means of contrastive losses.  These \"zero-shot\" methods are increasingly prevalent in the literature, and have the nice property that, unlike traditional ML classifiers, they don't need to be re-trained when new classes are added.  If we want to understand zero-shot methods, we may consider traditional classification *as an embedding method* of it own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Teaching ML: Viz, and Why 3D\n",
    "<br>\n",
    "<div style=\"display:inline-block; width:66%; float:left\">In teaching ML, it's common to teach <i>binary classification</i>* and then jump immediately into <i>multi-class</i> problems with large numbers of classes. This misses an opportunity for <i>visualization</i> -- the case of <b>3 classes</b>.<br><br>\n",
    "<p style=\"font-size:75%;\">*see my blog post <a href=\"https://hedges.belmont.edu/naughty\">\"Naughty by Numbers: Classifications at Christmas\"</a></p></div>\n",
    "<img src=\"images/naughty.png\" width=30% style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p style=\"color:#666; font-size:73%\"><br><br><br>I said no math, but:&nbsp;\n",
    "${\\rm softmax}(x_i) = {e^{x_i} \\over \\sum\\limits_j e^{x_j}} $&nbsp;. 3D gives you all of softmax's complexity &amp; you can still picture it!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Viz Matters for Teaching\n",
    "Data/sound viz has been an big part of my career, further influenced by Yang Hann Kim's 2016 Rossing Prize Lecture in Acoustics Education on viz in teaching STEM. \n",
    "<center><img src=\"images/my_viz_apps.png\" width=55%></center>\n",
    "Today I want to show you new work, including my \"Triangle Diagram\"! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Viz for > 3D = 😢\n",
    "<div style=\"float:right; width:50%; text-align:center; font-size:70%\"><img src=\"https://i.imgur.com/vKiH8As.png\" style=\"\">\n",
    "My blog post, \"<a href=\"https://drscotthawley.github.io/blog/2019/12/21/PCA-From-Scratch.html\">PCA from Scratch</a>\"\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"width:65%\">Humans can only visualize up to 3 dimensions, so for > 3  features we rely on <i>projections</i> via PCA or nonlinear embedding methods like t-SNE or UMAP. But with PCA data points tend to overlap, and other methods twist/distort/rip so global structure is lost.</div>\n",
    "\n",
    "\n",
    "In 3D the representations are *exact*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## New Code Library: [mrspuff](https://github.com/drscotthawley/mrspuff) \n",
    "\n",
    "For teaching via visualization and running on [Google Colab](https://colab.research.google.com), and leveraging [fast.ai](https://github.com/fastai/fastai).\n",
    "<center>\n",
    "    \n",
    "<img src=\"https://github.com/drscotthawley/mrspuff/raw/master/images/mrspuff_logo.png\" width=\"80%\">\n",
    "    <!--<p style=\"font-size:22pt; text-align:center;\"><a href=\"https://github.com/drscotthawley/mrspuff\">https://github.com/drscotthawley/mrspuff</a></p>-->\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "labels = ['cat','dog','horse']\n",
    "data = np.array([[0.7,0.2,0.1],[0.15,0.6,0.25],[0.05,0.15,0.8],[1,0,0],[0,1,0],[0,0,1]])\n",
    "\n",
    "# made the following to generate images to be loaded in the next cell. and save them to ./images/\n",
    "# You can ignore it\n",
    "if False:\n",
    "    from plotly.io import write_image\n",
    "    for i in range(len(labels)):\n",
    "        fig = image_and_bars(data[i], labels, CDH_SAMPLE_URLS[i]).show()\n",
    "        fname = f'images/{labels[i]}_bars.png'\n",
    "        try:\n",
    "            write_image(fig, fname) \n",
    "        except ValueError:\n",
    "            print(\"Sorry, you need the plotly-orca binary installed to auto-save images.\")\n",
    "            print(\"Click on the camera icon above to save manually to \",fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# save cropped versions; ignore it\n",
    "if False:\n",
    "    from PIL import Image\n",
    "    for i in range(len(labels)):\n",
    "        f = f'images/{labels[i]}'\n",
    "        fin = f+'_bars.png'\n",
    "        img = Image.open(fin)\n",
    "        img = img.crop((15,25,250,200))\n",
    "        img.save(f+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gA3XbAPN8-f_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensions and Embeddings\n",
    "\n",
    "Traditionally, we use triplets of numbers to denote 3 classes. \n",
    "\n",
    "\"Ground truth\" values are \"one-hot encoded\":<br>\n",
    "<pre>   cat: (1,0,0)           dog: (0,1,0)         horse: (0,0,1)</pre>\n",
    "\n",
    "Model predictics class probabilities for images:\n",
    "<div><img style=\"display:inline-block\" src=\"images/cat_bars.png\" width=\"32%\">\n",
    "    <img style=\"display:inline-block\" src=\"images/dog_bars.png\" width=\"33%\">\n",
    "    <img style=\"display:inline-block\" src=\"images/horse_bars.png\" width=\"33%\"></div>\n",
    "</p>\n",
    "...the 3 numbers always have to add up to 1 (probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "People who are not mathematicians, physicists, data scientists, etc. may be unaccustomed to this talk of \"dimensions\" when dealing with data. Let's dive in to the specific case of *three-class classification*.  Say we're developing a computer program to guess (\"predict\") whether  given image contains a cat, a dog, or a horse.  Traditionally we produce a set of 3 probabilities for each class, say...\n",
    "**TODO:** describe one-hot encoding of target values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhI52sWD9Lm-",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These numbers can be viewed as the strength of an attribute in an image, e.g. measures of cat-ness, dog-ness, and horse-ness (or measure of the likelihood of being a cat, dog, or horse, respectively), where a value of 1 means 100% of that property.  Notice in each case, the three \"class\" probabilities add up to 1.  This is always the case: probabilities always have to sum up to 1, i.e. 1 is \"100% certainty\" that gets split among the 3 classes.  (This summing to 1 is an important property that we'll come back to in a bit.)\n",
    "\n",
    "One thing that scientists like to do is take different variables and view them as *coordinates of a single point in a multi-dimensional space*.  So for 3 classes we have 3 coordinates for 3 dimensions.  We could make the \"cat-ness\" prediction probability be the \"x\" coordinate, and \"dog-ness\" be the \"y\" values, and \"horse-ness\" could be along the \"z\" axis.  Then instead of drawing bar graphs, we could plot points in 3D space, where the coordinates of each point tell us the predictions:\n",
    "\n",
    "*All the 3D plots in this post can be rotated & zoomed with the mouse or your finger. Try it!*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"Embedding\": treat each triplet as the (x,y,z) coordinates of a *point* in 3D space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "by32Mz4y9IRw",
    "outputId": "4a652786-4905-4bc0-c706-63d429306998",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TrianglePlot3D_Plotly(data, targ=None, labels=labels*2, show_bounds=False, nbdev=False).do_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18OLvGLS9QX7",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "(Here we also used the 3 class probabilities to set the R,G,B color values of the points. There's no new information contained in this; it just looks cool.)\n",
    "\n",
    "What scientists tend to do is, even in cases where there are more then 3 variables (say, 10), we regard these as dimensions in some fancy abstract mathematical space where the laws may or may not conform to those of our universe -- for example, the idea of \"distance\" may be totally up for grabs.  In cases where the number of values is infinite (say, as coefficients in a infinite series, or as a function of a continuous variable) we might even work in *infinite* dimensions!  Often when we talk like this, it doesn't mean that we're actually picturing geometrical spaces in our heads -- we can't, for anything beyond 3 dimensions -- but it's a handy way of encapsulating a particular way of viewing the data or functions involved.  And sometimes we *do* try to see what kinds of geometrical insights we can glean -- which is what we're going to do here!\n",
    "\n",
    "Remember when we said that the individual class probabilities have to add up to 1?  Look what happens when we plot a *lot* of such points..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's plot LOTS of points..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "hideCode": false,
    "id": "bz1dP26B9NyU",
    "outputId": "b73ad350-ff2f-43d0-cd32-93f8a80137de",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "prob, targ = calc_prob(n=400)\n",
    "TrianglePlot3D_Plotly(prob, targ=None, labels=labels, show_bounds=False).do_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtKuxjw5udRX",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that even though these are points in 3D space, they make up a *triangle* which lies along a *plane* -- a 2D :subspace\" of 3D.  This is a consequence of having the \"constraint\" that all class probabilities add up to 1. \n",
    "\n",
    "We can color the points by their expected class values by choosing the triangle point (or \"pole\") that they're nearest to -- i.e. by which \"bar\" is largest among the class probabilities.  And we can include the boundaries between classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's color them by their target value / label, and show class boundaries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "hideCode": false,
    "id": "1PgeVqXQuvfk",
    "outputId": "363d96ae-f917-4343-8296-8e68b8b9eafa",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TrianglePlot3D_Plotly(prob, targ=targ, labels=labels, show_bounds=True).do_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCqLI-jgu9Vi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3 Classes in 2D\n",
    "Since points lie in a plane, we can change coord's & plot in 2D instead<br>*(mouse hover = show image!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note: right now these images when you mouse over below are first scraped from DDG on-the-fly, and then rendered via `requests` when you mouse over.  Work in progress. Real-time tracking while training FastAI is on the TODO list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "hideCode": true,
    "id": "ExvL06A7wpvF",
    "outputId": "0a695323-14e2-422f-fe2c-f93aaed38eb0",
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "urls = exhibit_urls(targ, labels)\n",
    "TrianglePlot2D_Bokeh(prob, targ=targ, labels=labels, show_bounds=True, urls=urls).do_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrDhkiph9c5t",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Traditional Classification Training (Cartoon)\n",
    "<br>\n",
    "<div>\n",
    "    <img style=\"display:inline-block\" src=\"images/step1.png\" width=\"24%\">\n",
    "    <img style=\"display:inline-block\" src=\"images/step2.png\" width=\"24%\">\n",
    "    <img style=\"display:inline-block\" src=\"images/step3.png\" width=\"24%\">\n",
    "    <img style=\"display:inline-block\" src=\"images/step4.png\" width=\"24%\">\n",
    "</div>\n",
    "<div>\n",
    "    <img style=\"display:inline-block\" src=\"images/step5.png\" width=\"24%\">\n",
    "    <img style=\"display:inline-block\" src=\"images/step6.png\" width=\"24%\">\n",
    "    <img style=\"display:inline-block\" src=\"images/step7.png\" width=\"24%\">\n",
    "    <img style=\"display:inline-block\" src=\"images/step8.png\" width=\"24%\">\n",
    "</div>\n",
    "...tries to collapse all points to each \"pole\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# generate and save images that we'll load in the next cell \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate data along boundaries\n",
    "def gen_bound(x, y, z, n=20, ind0=1): # ind0=1 skips the \"first point\"\n",
    "    return np.linspace(np.array([x[0],y[0],z[0]]), np.array([x[1],y[1],z[1]]), num=n+ind0)[ind0:]\n",
    "  \n",
    "def gen_bound_data(n_per=20, ind0=0):\n",
    "    bdata = np.zeros((n_per*3,3))\n",
    "    bdata[:n_per] = gen_bound(x=[0.333,0.5], y=[0.333,0.5], z=[0.333,0], n=n_per, ind0=ind0) \n",
    "    bdata[n_per:2*n_per] = gen_bound(x=[0.333,0], y=[0.333,0.5], z=[0.333,0.5], n=n_per, ind0=ind0)\n",
    "    bdata[-n_per:] = gen_bound(x=[0.333,0.5], y=[0.333,0], z=[0.333,0.5], n=n_per, ind0=ind0)\n",
    "    return bdata\n",
    "\n",
    "def gen_near_bound_data(n_per=50, scale=7, eps=0.01):\n",
    "    bdata = gen_bound_data(n_per=n_per)\n",
    "    lower, right, left = bdata[0:n_per,:], bdata[n_per:2*n_per,:], bdata[-n_per:,:]\n",
    "\n",
    "    # shift data a bit\n",
    "    lower_catty = softmax( scale*(lower+np.array([eps,0,0])) )\n",
    "    lower_doggy = softmax( scale*(lower+np.array([0.0,eps,0])) )\n",
    "\n",
    "    left_catty = softmax( scale*(left+np.array([eps,0,0])) )\n",
    "    left_horsey = softmax( scale*(left+np.array([0,0,eps])) )\n",
    "\n",
    "    right_horsey = softmax( scale*(right+np.array([0,0,eps])) )\n",
    "    right_doggy = softmax( scale*(right+np.array([0,eps,0])) )\n",
    "\n",
    "    return np.vstack((lower_catty, lower_doggy, left_catty, left_horsey, right_horsey, right_doggy))\n",
    "\n",
    "# move boundary a bit toward the \"correct\" side\n",
    "eps = 0.007\n",
    "acc_data = gen_near_bound_data(eps=eps)\n",
    "btarg = np.argmax(acc_data, axis=-1)\n",
    "TrianglePlot2D_MPL(acc_data, targ=btarg, show_bounds=True, labels=labels, comment='100% Accuracy:').do_plot()\n",
    "plt.savefig(\"images/acc_100.png\")\n",
    "\n",
    "# move boundary a bit toward the \"wrong\" side (keeping labels the same as before)\n",
    "inacc_data = gen_near_bound_data(eps=-eps)\n",
    "ibtarg = btarg.copy()\n",
    "TrianglePlot2D_MPL(inacc_data, targ=ibtarg, show_bounds=True, labels=labels, comment='0% Accuracy:').do_plot()\n",
    "plt.savefig(\"images/acc_0.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Loss vs. Accuracy\n",
    "\n",
    "*Loss*: distance from target (continuous)<br>\n",
    "*Accuracy*: % of points on the correct side of decision boundary (discontinuous)<br>\n",
    "<br>\n",
    "**Nearly identical losses:**<br>\n",
    "<img style=\"display:inline-block\" src=\"images/acc_100.png\" width=\"40%\">\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img style=\"display:inline-block\" src=\"images/acc_0.png\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF2JYmvC9Wm5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4 Classes in 3D: form a pyramid!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "prob4, targ4 = calc_prob(n=500, s=2.7, dim=4)       # 4d probabilities\n",
    "prob4, targ4 = np.vstack((np.eye(4),prob4)), np.hstack((np.arange(4),targ4)) # tack on poles b4 pca\n",
    "prob3 = pca_proj(prob4)                     # use PCA for coordinate transformation to 3D hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "hideCode": true,
    "id": "6wspP59I9Tdg",
    "outputId": "7e5bcbb4-ee5f-442b-a048-69fbd55aa162",
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot = TrianglePlot3D_Plotly(prob3, targ=targ4, labels=labels+['bird'], show_labels=True, show_axes=False, poles_included=True)\n",
    "plot.fig.update_layout(scene_camera=dict( eye=dict(x=1.5, y=1, z=0.7)))\n",
    "plot.do_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYWmcVAf9kR5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Metric-Based Embedding Methods, OTOH...\n",
    "\n",
    "...map similar points near each other, dissimilar points far away. => Clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "def noop(x): return x \n",
    "\n",
    "def plot_clusters(dim=3, nclasses=4, nper=100, func=noop):\n",
    "    np.random.seed(6)\n",
    "    clusters = np.zeros((nclasses*nper,dim))\n",
    "    colors = ['red','green','blue','orange']+['black']*max(nclasses-4,0)\n",
    "    labels = ['cat','dog','horse','bird']+['aux']*max(nclasses-4,0)\n",
    "    fig = go.Figure()\n",
    "    for i in range(nclasses):\n",
    "        mean, cov = 0.8*np.random.rand(dim), 0.002*np.eye(dim)\n",
    "        cluster = func(np.random.multivariate_normal(mean, cov, nper))\n",
    "        clusters[i*nper:(i+1)*nper] = cluster\n",
    "        fig.add_trace( go.Scatter3d(x=cluster[:,0], y=cluster[:,1], z=cluster[:,2], hovertext=labels[i], name=labels[i], \\\n",
    "            mode='markers', marker=dict(size=5, opacity=0.6, color=colors[i])))\n",
    "    fig.update_layout(margin_t=0, scene_camera=dict( eye=dict(x=0.7, y=0.7, z=0.7)))\n",
    "    fig.show(config = {'displayModeBar': False})\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "hideCode": true,
    "id": "XF-LZfam9mqC",
    "outputId": "efbfbab7-e76c-441e-ac9c-5b4af15c4819",
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "clusters = plot_clusters() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgH1ZREO9qRS",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contrastive Loss\n",
    "Like attracts like; \"Opposites\" repel:\n",
    "    <img style=\"display:inline-block; vertical-align: text-top;\" src=\"images/CL_springs_transp.png\" width=\"50%\"><br>\n",
    "Tends to group things in \"semantically meaningful\" ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgH1ZREO9qRS",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This picture of springs is the essence of a \"[contrastive loss](https://arxiv.org/abs/2004.11362)\" function. Unlike traditional ML classification where the loss is based on the \"distance\" to a \"target\" (or \"ground truth\") value, with these metric based methods we send in two (or even 3) data points together, and then either let them attract or repel each other, and we do this over and over and over until we reach some stopping criterion.  Eventually, what we'll have is a space that contains clusters of similar points, separated by a \"margin\" distance that we specify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KgVawNp90FM",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Siamese Networks\n",
    "\n",
    "*Identical twin* network branches map to points (\"feature vectors\") in N-dim space:\n",
    "\n",
    "<center>\n",
    "<img src=\"https://people.kth.se/~rosun/deep-learning/figures/siamese-arch.svg\" width=\"65%\">\n",
    "    <p style=\"font-size:70%; text-align: center;\">Example of a Siamese Network (source: <a href=\"https://people.kth.se/~rosun/deep-learning\">Sundin et al</a>)</p>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQRVkgzy94qB",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How well do they work?\n",
    "\n",
    "Traditional vs. zero(/few)-shot methods: which one wins? *It depends.*\n",
    "\n",
    "From high-scoring Kaggle competition entry using \"[entity embeddings](https://arxiv.org/abs/1604.06737)\":\n",
    "\n",
    "> \"Entity embedding not only **reduces memory usage** and **speeds up neural networks** compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it **reveals the intrinsic properties of the categorical variables**.\"\n",
    "\n",
    "Let's look at \"PETS,\" via [my mod](https://colab.research.google.com/drive/1rSTOsj4coDu9mZCw57S7R6jBipm462R2?usp=sharing) of [FastAI's tutorial]() on Siamese Networks... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Zero-Shot & Few-Shot\n",
    "**Cool thing:** Embedding function tends to work for classes **never seen before** \n",
    "<img src=\"images/my_pets_preds.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/my_pets_preds2.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/my_pets_preds3.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "So, for example, the embedding learned for grouping images of cats, dogs, and horses together would map images of birds to nearby points in the space.  Then \"all we have to do\" if we want to predict a class is see whether a new instance is \"nearby\" (according to some distance measure we decide) to other similar points.  We could even look at the \"center points\" of various clusters and regard these as the \"class prototype\" and use that in the future.\n",
    "\n",
    "This fits (somewhat) with the notions of \"prototypes\" in human classification advanced by Eleanor Rosch in her revolutionary psychology work in the early 1970s.  We can say more about this later. ;-) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This same method of contrastive losses and metrics is used not for classification *per se* but for things like photographic identity verification (an example that is given in Andrew Ng's Machine Learning course on Coursera): Say you want to have a facial recognition system (highly problematic for ethical reasons but it's a good  example of the method so bear with me) for a company where there can be turnover in employees: You probably don't want to train a traditional classifier with separate a class for each employee because then you'd have to re-train it every time someone joins or leaves the company.  Instead, you can store an image of each employee, and then when they appear in front of a camera for identity verification, you could compare the \"distance\" between the embedded data point for the new photo from the data point for the stored photo(s).  If the distance is small enough, then you can have confidence it's the same person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAMnLUYKKUEJ",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " \n",
    "\n",
    "What's nice about this is that, after you've trained your embedding system, it can typically still be used to measure similarity between pairs of things it's never seen before, because in the process of training it was forced to learn \"semantically meaningful\" ways of grouping points together.  This use of the linguistic work \"semantic\" is not accidental: the language model systems that rely on \"word embeddings\" can learn to group similar words together, and even have mathematical-like relationships in analogies (e.g., gender: \"king - man + woman = queen\", or countries-and-capitals: \"Russia - Moscow + France = Paris\") by treating the embedded data points as *vectors* that point from the origin of the coordinate system to the data point.  We can say more about this and the distance metric they use (\"cosine similarity\") another time.\n",
    "\n",
    "\n",
    "So in using metric-based learning for classification, we're essentially adopting this identity-verification app and applying it to entire classes instead of individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMXCcMhI7E8U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final thoughts\n",
    "* 3D for teaching: robust yet viz-able, can help re. loss & accuracy\n",
    "* Embeddings are just mappings to points in space\n",
    "* Contrastive losses help give semantically meaningful embeddings\n",
    "* Note that real-world embeddings often require >> 3D (e.g. 128D)\n",
    "* `mrspuff` lib TODO: \n",
    "    * Live tracking of data points during training\n",
    "    * Real images for points \n",
    "        * Add URLs to FastAI image data type / bunch / loader\n",
    "\n",
    "## Acknowledgements\n",
    "Thanks to [Zach Mueller](https://twitter.com/TheZachMueller), [Tanishq Abraham](https://twitter.com/iScienceLuvr) and [Isaac Flath](https://twitter.com/isaac_flath) for help with fastai! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMXCcMhI7E8U",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## References:\n",
    "<ul style=\"font-size:90%\">\n",
    "<li> <a href=\"https://github.com/adambielski/siamese-triplet\">\"Siamese and triplet learning with online pair/triplet mining\"</a> by Adam Bielski\n",
    "<li> <a href=\"https://twitter.com/jeremyphoward/status/1133252425949753344/photo/1\">Example of new `Pipeline` class to create data for a Siamese model,\"</a> by Jeremy Howard\n",
    "<li> <a href=\"https://towardsdatascience.com/siamese-network-triplet-loss-b4ca82c1aec8\">\"Siamese Network &amp; Triplet Loss\"</a> by Rohith Gandhi\n",
    "<li> <a href=\"https://github.com/KevinMusgrave/pytorch-metric-learning\">pytorch-metric-learning</a> by Kevin Musgrave\n",
    "<li> <a href=\"https://towardsdatascience.com/contrastive-loss-explaned-159f2d4a87ec\">\"Contrastive Loss Explained\"</a> by Brian Williams\n",
    "<li> <a href=\"https://arxiv.org/abs/2006.07733\">\"Bootstrap Your Own Latent (BYOL)\"</a> by Grill et al\n",
    "<li> <b><a href=\"https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\">\"Siamese Neural Networks for One-shot Image Recognition\"</a> by Koch et al</b>\n",
    "<li> <a href=\"https://msiam.github.io/Few-Shot-Learning/\">\"Few Shot Learning\"</a> by msiam\n",
    "<li> <a href=\"https://katefvision.github.io/katefSlides/oneshotlearning_katef.pdf\">\"Learning to learn, Low shot learning\"</a> by Katerina Fragkiadaki\n",
    "<li> <a href=\"http://proceedings.mlr.press/v37/romera-paredes15.pdf\">\"Embarassingly Simple\": </a> <span style=\"color:blue\">\"We describe azero-shot learning approach that can be implemented in just one line of code, yet it is able to\n",
    "    outperform state of the art approaches on standard datasets\"</span>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Slideshow Instructions, Revisited:\n",
    "1. When the circle next to the \"Python 3\" in the upper right changes from being a solid circle to an open circle, that means `Kernel > Restart & Run All` has finished. After that...\n",
    "2. Start the show by pressing `Alt-R`* and then `Home`*.\n",
    "\n",
    "    \\* or the bar-graph icon in the toolbar. On a Mac, `Home` is `Command-LeftArrow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "2021-04-11-The-Joy-Of-3D.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
